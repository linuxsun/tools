
kubernetes cluster v1.14.3 

这是原始手工的部署方法，好处是，按这方法做一遍之后对部署流程就很熟悉了。

[root@kube1 ~]# cat /etc/redhat-release 
CentOS Linux release 7.6.1810 (Core) 
[root@kube1 ~]# uname -a                
Linux kube1 3.10.0-957.21.3.el7.x86_64 #1 SMP Tue Jun 18 16:35:19 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux

tee -a /etc/hosts <<-'EOF'
192.168.20.10 kube1 # master
192.168.20.11 kube2 # node1
192.168.20.12 kube3 # node2
EOF

cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF

#yum install kubeadm
# yum install kubelet  kubectl

#yum install kubeadm-1.11.1 kubectl-1.11.1 
yum install kubelet-1.14.3 kubectl-1.14.3 kubeadm-1.14.3

##############################

images=(
  kube-apiserver:v1.14.3
  kube-controller-manager:v1.14.3
  kube-scheduler:v1.14.3
  kube-proxy:v1.14.3
  pause:3.1
  etcd:3.3.10
  coredns:1.3.1
)
k8s.gcr.io/kube-apiserver:v1.14.3
k8s.gcr.io/kube-controller-manager:v1.14.3
k8s.gcr.io/kube-scheduler:v1.14.3
k8s.gcr.io/kube-proxy:v1.14.3
k8s.gcr.io/pause:3.1
k8s.gcr.io/etcd:3.3.10
k8s.gcr.io/coredns:1.3.1

for in in ${images[@]} ; do
    docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/$in
    docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/$in k8s.gcr.io/$in
	docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/$in
done


链接：https://pan.baidu.com/s/1NrHRSSKhKdARLp8y-Baz5Q 
提取码：uxv2 

# --cgroup-driver=systemd
touch /etc/sysconfig/kubelet
echo 'KUBELET_EXTRA_ARGS="--cgroup-driver=systemd"' > /etc/sysconfig/kubelet
systemctl restart {docker,kubelet}
systemctl status {docker,kubelet} -l

kubeadm init --pod-network-cidr=10.0.0.0/8 --kubernetes-version=v1.14.3 --apiserver-advertise-address=192.168.20.10
......
[kubelet] Creating a ConfigMap "kubelet-config-1.14" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --experimental-upload-certs
[mark-control-plane] Marking the node kube1 as control-plane by adding the label "node-role.kubernetes.io/master=''"
[mark-control-plane] Marking the node kube1 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: 9ov650.j4nk608oi5rqaaa1
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstraptoken] creating the "cluster-info" ConfigMap in the "kube-public" namespace
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy
Your Kubernetes master has initialized successfully!
To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of machines by running the following on each node
as root:
kubeadm join 192.168.204.10:6443 --token obq630.1galcwg8y4b2alq6 \
    --discovery-token-ca-cert-hash sha256:2b312b9eb010b034584cc80bcdb1ad99547597047014b2146e454c763e4c5461 


export KUBECONFIG=/etc/kubernetes/admin.conf
echo "export KUBECONFIG=/etc/kubernetes/admin.conf" >> /etc/profile


# weave网络插件配置文件.
# 此步骤在kubeadm init之前操作更好
test -f /etc/cni/net.d/10-weave.conflist || \
cat > /etc/cni/net.d/10-weave.conflist <<-'EOF'
{
    "cniVersion": "0.3.0",
    "name": "weave",
    "plugins": [
        {
            "name": "weave",
            "type": "weave-net",
            "hairpinMode": true
        },
        {
            "type": "portmap",
            "capabilities": {"portMappings": true},
            "snat": true
        }
    ]
}
EOF

# 安装weave网络插件
# 此步骤在kubeadm init之前操作更好
curl -fsSLo weave-daemonset.yaml "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"
kubectl apply -f weave-daemonset.yaml

# weave网络插件未安装好之前，状态会一直处于NotReady
kubectl get nodes
kubectl get nodes --show-labels
NAME      STATUS     ROLES     AGE       VERSION
master    NotReady   master    1d        v1.14.3

kubectl describe node kube1

# 查看admin-user-token密钥文件
kubectl get secret --all-namespaces
kubectl describe secrets -n kube-system admin-user-token-6fhwn



#删除一个pods
kubectl delete pods kubernetes-dashboard-7d75c474bb-g7xzm

# 查看集群信息
kubectl cluster-info

#查看 Pods 信息
kubectl get pods --all-namespaces

#
kubectl get pod -n kube-system
kubectl describe pod -n kube-system weave-net-chhpd

#重新生成 token kube1
#kubeadm token generate
#kubeadm token create <generated-token> --print-join-command --ttl=24h

# 设置Master同时也是nodes节点，并能接收部署pods
kubectl label node kube1 node-role.kubernetes.io/node=

# 为节点添加"NodeType=storage"标签
kubectl label nodes kube2 NodeType=storage

# 获取token值.
默认情况下，令牌在24小时后过期
kubeadm token list
TOKEN                     TTL       EXPIRES                     USAGES                   DESCRIPTION                                                EXTRA GROUPS
9ov650.j4nk608oi5rqaaa1   23h       2019-07-10T00:02:02+08:00   authentication,signing   The default bootstrap token generated by 'kubeadm init'.   system:bootstrappers:kubeadm:default-node-token



（可选）将API服务器代理到localhost
如果要从群集外部连接到API服务器，可以使用 kubectl proxy：

scp root@<master ip>:/etc/kubernetes/admin.conf .
kubectl --kubeconfig ./admin.conf proxy
# kubectl proxy --address=192.168.20.12 --disable-filter=true
您现在可以在本地访问API服务器 http://localhost:8001/api/v1



################
安装可视化插件kubernetes-dashboard

该插件可能会随机选中集群中的一台节点去部署dashboard插件容器，所以ssh登录到该节点，先手工把镜像获取下来。
docker pull kubernetesui/dashboard:v2.0.0-beta1
docker pull kubernetesui/metrics-scraper:v1.0.0
wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta1/aio/deploy/recommended.yaml
kubectl create -f recommended.yaml


tee dashboard-adminuser.yaml <<-'EOF'
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kubernetes-dashboard
EOF

tee admin-user-role-binding.yaml <<-'EOF'
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kubernetes-dashboard
EOF

kubectl create -f dashboard-adminuser.yaml 
kubectl create -f admin-user-role-binding.yaml

获取Token
kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk '{print $1}')

kubectl get pods --all-namespaces
NAMESPACE              NAME                                          READY     STATUS    RESTARTS   AGE
default                nginx-deployment-5c678cfb6d-6mlnj             1/1       Running   0          2h
default                nginx-deployment-5c678cfb6d-ln9c9             1/1       Running   0          2h
kube-system            coredns-78fcdf6894-46j2d                      1/1       Running   3          3d
......
kube-system            kube-scheduler-kube1                          1/1       Running   4          3d
kube-system            weave-net-86lg2                               2/2       Running   12         3d
kube-system            weave-net-r5dbz                               2/2       Running   8          3d
kubernetes-dashboard   kubernetes-dashboard-7f9898654f-x7495         1/1       Running   0          1h
kubernetes-dashboard   kubernetes-metrics-scraper-7b9df76cdb-pngxq   1/1       Running   0          1h

kubectl cluster-info
Kubernetes master is running at https://192.168.20.10:6443
KubeDNS is running at https://192.168.20.10:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.



kubectl get -f recommended.yaml
NAME                   STATUS    AGE
kubernetes-dashboard   Active    1h
NAME                   SECRETS   AGE
kubernetes-dashboard   1         1h
NAME                   TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
kubernetes-dashboard   ClusterIP   10.109.221.42   <none>        443/TCP   1h
......
NAME                        TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
dashboard-metrics-scraper   ClusterIP   10.100.197.108   <none>        8000/TCP   1h\
NAME                         DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
kubernetes-metrics-scraper   1         1         1            1           1h


kubectl --namespace=kubernetes-dashboard logs kubernetes-dashboard-7f9898654f-x7495
......
2019/07/12 16:00:42 [2019-07-12T16:00:42Z] Incoming HTTP/2.0 GET /api/v1/pod/default/nginx-deployment-5c678cfb6d-6mlnj/event?itemsPerPage=10&page=1 request from 10.32.0.1:52730: 
2019/07/12 16:00:42 Getting details of nginx-deployment-5c678cfb6d-6mlnj pod in default namespace
2019/07/12 16:00:42 Getting events related to a pod in namespace
2019/07/12 16:00:42 [2019-07-12T16:00:42Z] Outcoming response to 10.32.0.1:52730 with 200 status code
2019/07/12 16:00:42 No persistentvolumeclaims found related to nginx-deployment-5c678cfb6d-6mlnj pod
2019/07/12 16:00:42 [2019-07-12T16:00:42Z] Outcoming response to 10.32.0.1:52730 with 200 status code


k8s默认启用RBAC
创建一个证书：

使用client-certificate-data和client-key-data生成一个p12文件：
# 生成client-certificate-data
grep 'client-certificate-data' /etc/kubernetes/admin.conf | head -n 1 | awk '{print $2}' | base64 -d >> kubecfg.crt

# 生成client-key-data
grep 'client-key-data' /etc/kubernetes/admin.conf | head -n 1 | awk '{print $2}' | base64 -d >> kubecfg.key

# 生成kubecfg.p12
openssl pkcs12 -export -clcerts -inkey kubecfg.key -in kubecfg.crt -out kubecfg.p12 -name "kubernetes-client"

#最后导入上面生成的p12文件，重新打开谷歌浏览器。(建议用谷歌浏览器。)
证书导入向导-->证书存储-->选择：根据证书类型，自动选择证书存储-->下一步-->完成-->导入成功-->关闭-->重启浏览器


访问Kubernetes Dashboard选择 -->Token-->输入Token-->登录
获取Token：kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk '{print $1}')
https://192.168.20.10:6443/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/#/login


tee get_token.sh <<-'EOF'
HTTPS=$(kubectl cluster-info|grep -e 'Kubernetes master'|awk '{print $6}')
URI='api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/#/login'
URL="$HTTPS/$URI"
echo "URL: $URL"
kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk '{print $1}')|grep -E "(Namespace|Annotations|token)"
EOF

chmod 750 ./get_token.sh && ./get_token.sh


####################

部署容器存储插件
Rook 项目是一个基于 Ceph 的 Kubernetes 存储插件 生产级别可用的容器存储插件。 

# 在kube1(Master)执行以下指令：
docker pull rook/ceph:master
docker pull ceph/daemon-base:latest-nautilus-devel

docker save -o rook_ceph.tar rook/ceph:master
docker save -o ceph_daemon-base.tar ceph/daemon-base

分发镜像到kube2地点和kube3节点：
scp rook_ceph.tar kube2:~/rook_ceph.tar
scp ceph_daemon-base.tar kube2:~/ceph_daemon-base.tar
scp rook_ceph.tar kube3:~/rook_ceph.tar
scp ceph_daemon-base.tar kube3:~/ceph_daemon-base.tar

ssh kube2 'docker load < ~/rook_ceph.tar'
ssh kube2 'docker load < ~/ceph_daemon-base.tar'
ssh kube3 'docker load < ~/rook_ceph.tar'
ssh kube3 'docker load < ~/ceph_daemon-base.tar'

#
wget https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/common.yaml
wget https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/operator.yaml
wget https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/cluster.yaml

# 创建一个rook-ceph集群。
kubectl create -f common.yaml
kubectl create -f operator.yaml

# 当以下pod都正常running之后，再执行后续步骤: kubectl create -f cluster.yaml
# verify the rook-ceph-operator, rook-ceph-agent, and rook-discover pods are in the `Running` state before proceeding
kubectl -n rook-ceph get pod


# 每个节点的“名称”字段应与其“kubernetes.io/hostname”标签相匹配。
# - name: "kube3" #

cat cluster-add-disk.yaml |grep -Ev '(^#|*.#)'
apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  cephVersion:
    image: ceph/daemon-base:latest-nautilus-devel
    allowUnsupported: false
  dataDirHostPath: /var/lib/rook
  mon:
    count: 3
    allowMultiplePerNode: false
  dashboard:
    enabled: true
  monitoring:
    enabled: false
    rulesNamespace: rook-ceph
  network:
    hostNetwork: false
  rbdMirroring:
    workers: 0
  annotations:
  resources:
    useAllNodes: false
    useAllDevices: false
    deviceFilter:
    location:
    config:
    directories:
    - path: /var/lib/rook
    nodes:
    - name: "kube2"
      devices:
      - name: "sdd"
      config:
        storeType: bluestore
        osdsPerDevice: "5"
    - name: "kube3"
      devices:
      - name: "sdd"
      config:
        storeType: bluestore
        osdsPerDevice: "5"
    - name: "kube4"
      devices:
      - name: "sdd"
      config:
        storeType: bluestore
        osdsPerDevice: "5"


kubectl create -f cluster.yaml



kubectl get pods --all-namespaces    
kubernetes-dashboard   kubernetes-dashboard-6f89577b77-fzbgv         1/1     Running   4          41h
kubernetes-dashboard   kubernetes-metrics-scraper-79c9985bc6-7bj4s   1/1     Running   2          41h
rook-ceph              rook-ceph-agent-hrrzz                         1/1     Running   0          24m
rook-ceph              rook-ceph-agent-tskg2                         1/1     Running   0          24m
rook-ceph              rook-ceph-operator-775cf575c5-g4s97           1/1     Running   0          41m
rook-ceph              rook-discover-7zp9v                           1/1     Running   0          24m
rook-ceph              rook-discover-dz5lc                           1/1     Running   0          24m

kubectl describe pod -n rook-ceph rook-ceph-detect-version-vg9tv
kubectl describe pod -n rook-ceph rook-discover-7zp9v

#kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')

kubectl get ns
kubectl get pod -n rook-ceph -o wide
kubectl -n rook-ceph get deployment


kubectl get pods -n rook-ceph
NAME                                  READY   STATUS    RESTARTS   AGE
rook-ceph-agent-6n857                 1/1     Running   0          26m
rook-ceph-agent-b24kb                 1/1     Running   1          79m
rook-ceph-operator-775cf575c5-llxsd   1/1     Running   0          56m
rook-discover-sj5dq                   1/1     Running   0          26m
rook-discover-smzsq                   1/1     Running   1          79m

kubectl get service -n rook-ceph | grep dashboard

# rook-ceph-mon的IP地址正确获取方式
[root@kube1 ~]# kubectl -n rook-ceph get service
NAME                                     TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE
rook-ceph-mgr                            ClusterIP   10.98.35.201     <none>        9283/TCP            46h
rook-ceph-mgr-dashboard                  ClusterIP   10.107.150.91    <none>        8443/TCP            46h
rook-ceph-mgr-dashboard-external-https   NodePort    10.101.74.39     <none>        8443:31169/TCP      46h
rook-ceph-mon-a                          ClusterIP   10.108.224.132   <none>        6789/TCP,3300/TCP   46h
rook-ceph-mon-b                          ClusterIP   10.103.69.118    <none>        6789/TCP,3300/TCP   46h
rook-ceph-mon-c                          ClusterIP   10.97.179.87     <none>        6789/TCP,3300/TCP   46h
rook-ceph-mon-d                          ClusterIP   10.98.44.174     <none>        6789/TCP,3300/TCP   45h


wget https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/toolbox.yaml
kubectl apply -f toolbox.yaml
[root@kube1 ~]# kubectl -n rook-ceph get pods -o wide | grep ceph-tools
rook-ceph-tools-b8c679f95-lbfwz       1/1     Running     0          28s     192.168.90.245   kube2   <none>           <none>
kubectl -n rook-ceph exec -it rook-ceph-tools-b8c679f95-lbfwz sh
sh-4.2# ceph status
  cluster:
    id:     4369be5c-46f8-42ce-9763-728807ae1594
    health: HEALTH_WARN
            1 osds down
  services:
    mon: 3 daemons, quorum a,b,c (age 3m)
    mgr: a(active, since 13m)
    osd: 6 osds: 5 up (since 3m), 6 in (since 3m)
  data:
    pools:   0 pools, 0 pgs
    objects: 0 objects, 0 B
    usage:   37 GiB used, 131 GiB / 168 GiB avail
    pgs:     
exit

kubectl apply -f dashboard-loadbalancer.yaml
kubectl apply -f dashboard-ingress-https.yaml
kubectl apply -f dashboard-external-http.yaml 
kubectl apply -f dashboard-external-https.yaml

kubectl get service -n rook-ceph | grep dashboard
rook-ceph-mgr-dashboard                  ClusterIP   10.107.150.91    <none>        8443/TCP            3m58s
rook-ceph-mgr-dashboard-external-https   NodePort    10.101.74.39     <none>        8443:31169/TCP      18s
访问任意节点IP： http://IP:31169

#获取rook-ceph密码
MGR_POD=`kubectl get pod -n rook-ceph | grep mgr | awk '{print $1}'`
kubectl -n rook-ceph logs $MGR_POD | grep password


wget https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/storageclass.yaml
kubectl apply -f storageclass.yaml
cat storageclass.yaml 
#################################################################################################################
# Create a storage class with a pool that sets replication for a production environment.
# A minimum of 3 nodes with OSDs are required in this example since the default failureDomain is host.
#  kubectl create -f storageclass.yaml
#################################################################################################################

apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: replicapool
  namespace: rook-ceph
spec:
  replicated:
    size: 3
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-ceph-block
provisioner: ceph.rook.io/block
parameters:
  blockPool: replicapool
  # Specify the namespace of the rook cluster from which to create volumes.
  # If not specified, it will use `rook` as the default namespace of the cluster.
  # This is also the namespace where the cluster will be
  clusterNamespace: rook-ceph
  # Specify the filesystem type of the volume. If not specified, it will use `ext4`.
  fstype: xfs
  # (Optional) Specify an existing Ceph user that will be used for mounting storage with this StorageClass.
  #mountUser: user1
  # (Optional) Specify an existing Kubernetes secret name containing just one key holding the Ceph user secret.
  # The secret must exist in each namespace(s) where the storage will be consumed.
  #mountSecret: ceph-user1-secret


[root@kube1 ~]# kubectl get storageclass
NAME              PROVISIONER          AGE
rook-ceph-block   ceph.rook.io/block   27m

wget https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/mysql.yaml
wget https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/wordpress.yaml
kubectl apply -f mysql.yaml
kubectl apply -f wordpress.yaml

[root@kube1 ~]# kubectl get pvc
NAME             STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE
mysql-pv-claim   Bound    pvc-3c93e5e4-b04b-11e9-9b13-005056a86b9c   1Gi        RWO            rook-ceph-block   23m
wp-pv-claim      Bound    pvc-86a7e1f6-b04b-11e9-9b13-005056a86b9c   512Mi      RWO            rook-ceph-block   21m

kubectl get pv

kubectl -n rook-ceph get pods -o wide | grep ceph-tools
rook-ceph-tools-b8c679f95-zxs4d       1/1     Running     0          28m   192.168.90.245   kube2   <none>           <none>
kubectl -n rook-ceph exec -it  rook-ceph-tools-b8c679f95-zxs4d  sh
sh-4.2# rbd info -p replicapool pvc-3c93e5e4-b04b-11e9-9b13-005056a86b9c
rbd image 'pvc-3c93e5e4-b04b-11e9-9b13-005056a86b9c':
        size 1 GiB in 256 objects
        order 22 (4 MiB objects)
sh-4.2# exit


登陆pod检查rbd设备
[root@kube1 ~]# kubectl get pod -o wide
NAME                                READY   STATUS    RESTARTS   AGE   IP          NODE    NOMINATED NODE   READINESS GATES
nginx-deployment-57bb869b8b-4942f   1/1     Running   3          24h   10.42.0.1   kube3   <none>           <none>
nginx-deployment-57bb869b8b-xm25n   1/1     Running   2          24h   10.36.0.1   kube4   <none>           <none>
wordpress-595685cc49-rtg4z          1/1     Running   0          23m   10.36.0.6   kube4   <none>           <none>
wordpress-mysql-b78774f44-xx9lm     1/1     Running   0          25m   10.44.0.4   kube2   <none>           <none>
[root@kube1 ~]# kubectl exec -it wordpress-595685cc49-rtg4z  bash
root@wordpress-595685cc49-rtg4z:/var/www/html# mount|grep rbd
/dev/rbd0 on /var/www/html type xfs (rw,relatime,attr2,inode64,sunit=8192,swidth=8192,noquota)


#重要
由于RBD映射在主机系统上。主机需要能够解析由kube-dns服务管理的ceph-mon.ceph.svc.cluster.local名称。
要获取kube-dns服务的IP地址，请运行:
kubectl -n kube-system get svc/kube-dns
# https://docs.ceph.com/docs/master/start/kube-helm/


rook参考：
https://blog.csdn.net/networken/article/details/85772418
https://github.com/rook/rook/wiki/Storage-Configuration-Design
https://docs.ceph.com/docs/master/start/
https://rook.io/docs/rook/v1.0/ceph-quickstart.html


#kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')


##################################################
# Service与Ingress 
所谓 Ingress，就是 Service 的“Service”。
所谓 Ingress 对象，其实就是 Kubernetes 项目对“反向代理”的一种抽象。
我如何能使用 Kubernetes 的 Ingress来创建一个统一的负载均衡器，从而实现当用户访问不同的域名时，能够访问到不同的 Deployment 呢？


一个 Nginx Ingress Controller 为你提供的服务，其实是一个可以根据 Ingress 对象和被代理后端Service 的变化，来自动进行更新的 Nginx 负载均衡器。 
wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml
kubectl apply -f mandatory.yaml


当然，为了让用户能够用到这个 Nginx，我们就需要创建一个 Service来把 Nginx Ingress Controller管理的 Nginx 服务暴露出去： 
wget  https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/baremetal/service-nodeport.yaml
kubectl apply -f service-nodeport.yaml


https://github.com/resouer/kubernetes-ingress/tree/master/examples/complete-example


# kube Service、DNS与服务发现
Service 是由 kube-proxy 组件，加上 iptables 来共同实现的。 
-A KUBE-SERVICES -d 10.0.1.175/32 -p tcp -m comment --comment "default/hostnames: cluster IP" -m tcp --dport 80 -j KUBE-SVC-NWV5X2332I4OT4T3

<pod-name>.<svc-name>.<namespace>.svc.cluster.local
nginx-deployment.yaml

kubectl create -f svc.yaml
$ kubectl get service nginx
NAME      TYPE         CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
nginx     ClusterIP    None         <none>        80/TCP    10s

$ kubectl create -f statefulset.yaml
$ kubectl get statefulset web
NAME      DESIRED   CURRENT   AGE
web       2         1         19s


kubectl get pods -w -l app=nginx
NAME      READY     STATUS    RESTARTS   AGE
web-0     0/1       Pending   0          0s
web-0     0/1       Pending   0         0s
web-0     0/1       ContainerCreating   0         0s
web-0     1/1       Running   0         19s
web-1     0/1       Pending   0         0s
web-1     0/1       Pending   0         0s
web-1     0/1       ContainerCreating   0         0s
web-1     1/1       Running   0         20s


kubectl exec web-0 -- sh -c 'hostname'
web-0
$ kubectl exec web-1 -- sh -c 'hostname'
web-1

nameserver 10.96.0.10

kubectl run -i --tty --image busybox dns-test --restart=Never --rm /bin/sh 
$ nslookup web-0.nginx
Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

Name:      web-0.nginx
Address 1: 10.244.1.8

$ nslookup web-1.nginx
Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

Name:      web-1.nginx
Address 1: 10.244.2.8


kubectl create -f statefulset.yaml
$ kubectl get pvc -l app=nginx
NAME        STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE
www-web-0   Bound     pvc-15c268c7-b507-11e6-932f-42010a800002   1Gi        RWO           48s
www-web-1   Bound     pvc-15c79307-b507-11e6-932f-42010a800002   1Gi        RWO           48s

web-0.nginx.default.svc.cluster.local
nginx-deployment-57bb869b8b-zrclq.nginx.default.svc.cluster.local


# busybox可能无法解释出A记录，换成tutum/dnsutils镜像
kubectl run -i --tty --image tutum/dnsutils dns-test --restart=Never --rm /bin/sh

# 在被重新创建出来的 Pod 容器里访问 http://localhost
$ kubectl exec -it web-0 -- curl localhost
hello web-0

kubectl logs -f coredns-fb8b8dccf-h5t2h   -n kube-system  

kubectl run -i --tty --image tutum/dnsutils dns-test --restart=Never --rm /bin/sh 

# 在宿主机上执行,打开 iptables 的 TRACE 功能查看到数据包的传输过程 
iptables -t raw -A OUTPUT -p icmp -j TRACE
iptables -t raw -A PREROUTING -p icmp -j TRACE


[root@kube1 ~]#  kubectl get svc -n ingress-nginx
NAME            TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
ingress-nginx   NodePort   10.102.247.199   <none>        80:31396/TCP,443:31942/TCP   5h42m

kubectl get svc --all-namespaces
default                kubernetes                               ClusterIP      10.96.0.1        <none>        443/TCP                      9d
kube-system            kube-dns                                 ClusterIP      10.96.0.10       <none>        53/UDP,53/TCP,9153/TCP       9d
default                wordpress                                LoadBalancer   10.103.98.220    <pending>     80:30354/TCP                 8d


kubectl get ingress
kubectl describe ingress cafe-ingress

IC_IP=192.168.204.12
IC_HTTPS_PORT=31942 # 通过该指令获得kubectl get svc -n ingress-nginx

# --insecure假证书
curl --resolve cafe.example.com:$IC_HTTPS_PORT:$IC_IP https://cafe.example.com:$IC_HTTPS_PORT/coffee --insecure
Server name: coffee-7dbb5795f6-vglbv
Date: 03/Nov/2018:03:55:32 +0000
URI: /coffee
Request ID: e487e672673195c573147134167cf898

curl -v -s -k --key certs/client.key --cert certs/client.crt https://127.0.0.1:8043


### Ingress Controller 和它所需要的Service 部署完成后，如何使用？
wget https://raw.githubusercontent.com/resouer/kubernetes-ingress/master/examples/complete-example/cafe-ingress.yaml
wget https://raw.githubusercontent.com/resouer/kubernetes-ingress/master/examples/complete-example/cafe-secret.yaml
wget https://raw.githubusercontent.com/resouer/kubernetes-ingress/master/examples/complete-example/cafe.yaml

kubectl apply -f cafe-ingress.yaml
kubectl apply -f cafe-secret.yaml
kubectl apply -f cafe.yaml


###### ingress-nginx 另外一种部署方法 ######
wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml
sed -i 's|quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.25.1|registry.cn-hangzhou.aliyuncs.com/kubeapps/quay-nginx-ingress-controller:0.24.1|g' mandatory.yaml  
kubectl apply -f mandatory.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/baremetal/service-nodeport.yaml

POD_NAMESPACE=ingress-nginx
POD_NAME=$(kubectl get pods -n $POD_NAMESPACE -l app.kubernetes.io/name=ingress-nginx -o jsonpath='{.items[0].metadata.name}')
kubectl exec -it $POD_NAME -n $POD_NAMESPACE -- /nginx-ingress-controller --version

#官网
https://kubernetes.github.io/ingress-nginx/deploy/


###### Service 与 DNS 的关系

在 Kubernetes 中，Service 和Pod 都会被分配对应的DNS A 记录（从域名解析 IP 的记录）。

对于 ClusterIP 模式的 Service 来说（比如我们上面的例子），它的 A 记录的格式是：..svc.cluster.local。当你访问这条 A 记录的时候，它解析到的就是该 Service 的 VIP 地址。

而对于指定了 clusterIP=None 的Headless Service 来说，它的 A 记录的格式也是：..svc.cluster.local。但是，当你访问这条 A 记录的时候，它返回的是所有被代理的 Pod 的 IP 地址的集合。 

此外，对于 ClusterIP 模式的 Service 来说，它代理的 Pod 被自动分配的A 记录的格式是：..pod.cluster.local。这条记录指向 Pod 的 IP 地址。 


####### helm #########

## 安装客户端 helm
wget -qO- --no-check-certificate https://get.helm.sh/helm-v2.12.3-linux-amd64.tar.gz | tar xz -C /usr/local/bin
sed -i 's|PATH=$PATH:$HOME/bin|PATH=$PATH:$HOME/bin:/usr/local/bin/linux-amd64|' ~/.bash_profile
source ~/.bash_profile

## 安装服务端 tiller
前提是已经部署了kubernetes集群

Kubernetes会随机将tiller-deploy-xxx-XYZ-xxx分配到任意一台宿主主机运行，
可以在控制台上查看，然后以下指令就在相应的宿主主机上执行：
docker pull fishead/gcr.io.kubernetes-helm.tiller:v2.12.3 
docker tag fishead/gcr.io.kubernetes-helm.tiller:v2.12.3 gcr.io/kubernetes-helm/tiller:v2.12.3

以下指令在Kubernetes Master执行:
helm init --service-account=tiller --tiller-image=gcr.io/kubernetes-helm/tiller:v2.12.3  --history-max 300

# or
helm init --client-only --stable-repo-url https://apphub.aliyuncs.com

tee -a helm-rbac.yaml <<-'EOF'
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: tiller
    namespace: kube-system
EOF

# kubectl apply -f helm-rbac.yaml
# kubectl get deployment tiller-deploy -n kube-system	
NAME            READY   UP-TO-DATE   AVAILABLE   AGE
tiller-deploy   1/1     1            1           44m

# kubectl get pods -n kube-system|grep tiller
tiller-deploy-8cdc9f559-j7s8t   1/1     Running   0          10m

helm install stable/nginx-ingress --name nginx-ingress
helm ls
helm repo update #更新chart repo

##创建一个chart
helm create hello-svc
cd hello-svc
helm install --dry-run --debug ./
helm install ./
helm list
NAME                    REVISION        UPDATED                         STATUS          CHART           APP VERSION     NAMESPACE
dangling-ragdoll        1               Wed Aug 14 17:11:50 2019        DEPLOYED        hello-svc-0.1.0 1.0             default 

## Helm Hub “中国站”
helm repo add apphub https://apphub.aliyuncs.com
helm search apphub

## 启动服务端
export HELM_HOST=localhost:44134
/usr/local/bin/linux-amd64/tiller >/dev/null 2>&1 &
 

##删除release
helm delete dangling-ragdol

##chart打包分发
[root@kube1 hello-svc]# helm package ./
[root@kube1 hello-svc]# ls
charts  Chart.yaml  hello-svc-0.1.0.tgz  templates  values.yaml

## 删除或重新安装
$ kubectl get all --all-namespaces | grep tiller
$ kubectl delete deployment tiller-deploy -n kube-system
$ kubectl delete service tiller-deploy -n kube-system
$ kubectl get all --all-namespaces | grep tiller
$ helm reset



## 参考
1. https://github.com/helm/charts
2. https://github.com/cloudnativeapp/charts
3. https://www.kubernetes.org.cn/3435.html
4. https://helm.sh/docs/using_helm/#installing-helm
5. https://devopscube.com/install-configure-helm-kubernetes/
6. https://www.jianshu.com/p/d0cdbb49569b 
7. https://github.com/jay-johnson/deploy-to-kubernetes/tree/master/redis
8. https://stackoverflow.com/questions/51646957/helm-could-not-find-tiller

##### 设置存储类#######
## 空输出意味着没有存储类。
kubectl describe sc

建议为Digital Ocean 安装CSI-driver。这将do-block-storage使用Kubernetes CSI接口创建一个类。

另一种选择是使用本地存储。使用本地存储类：
$ cat <<EOF
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
EOF | kubectl apply -f -

然后，对于这两种情况，如果未storageClassName在PVC中指定，则可能需要将其设置为默认存储类：
$ kubectl patch storageclass local-storage -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'

要么:
$ kubectl patch storageclass do-block-storage -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'


参考：
https://stackoverflow.com/questions/52975887/digitalocean-pod-has-unbound-immediate-persistentvolumeclaims


####### 基于helm部署redis集群 ######

helm install --name redis-master1-slave3 stable/redis --values values-production.yaml
kubectl run --namespace default redis-master1-slave3-client --rm --tty -i --restart='Never' \
    --env REDIS_PASSWORD=$REDIS_PASSWORD \--labels="redis-master1-slave3-client=true" \
   --image docker.io/bitnami/redis:5.0.5-debian-9-r104 -- bash

## redis 集群安装参考
https://github.com/helm/charts/tree/master/stable/redis
https://engineering.bitnami.com/articles/deploy-and-scale-a-redis-cluster-on-kubernetes-with-bitnami-and-helm.html

注意：
1) 如果自定义存储类，要先创建  kubectl apply -f pvc-ceph.yml 
cat pvc-ceph.yml 
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: redis-ceph-data
  namespace: default
  labels:
    app: redis
    release: redis
    role: master
    storage: ceph
spec:
  storageClassName: rook-ceph-block
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 4Gi

2)values-production.yaml 部份值要做调整
persistence:
  existingClaim: redis-ceph-data
master:
  persistence:
    enabled: true
  storageClassName: rook-ceph-block
slave:
  persistence:
  enabled: false #从节点只读，不需要持久化

3) NetworkPolicy
要为Redis启用网络策略，请安装 实现Kubernetes NetworkPolicy规范的网络插件，并设置networkPolicy.enabled为true。
对于Kubernetes v1.5和v1.6，您还必须通过设置DefaultDeny名称空间注释来打开NetworkPolicy。注意：这将对命名空间中的所有 pod 强制执行策略：
kubectl annotate namespace default "net.beta.kubernetes.io/network-policy={\"ingress\":{\"isolation\":\"DefaultDeny\"}}"
启用NetworkPolicy后，只有具有生成的客户端标签的pod才能连接到Redis。成功安装后，此标签将显示在输出中。



参考：
https://github.com/rancher/rancher/issues/13442
https://helm.sh/docs/using_helm/#installing-helm
https://github.com/jay-johnson/deploy-to-kubernetes/tree/master/redis


###### kubernetes cluster 常用指令 ######

查看 ENDPOINTS IP地址
kubectl get ep

查看pvc及删除pvc
kubectl get pvc
kubectl delete pvc <rabbitmq-name>

#Rabbitmq 添加 vhosts
kubectl port-forward --namespace default svc/rabbitmq 15672:15672
curl -u user:ZE5ujZaUs4 -X PUT http://127.0.0.1:15672/api/vhosts/Rabbitmq
#参考https://www.rabbitmq.com/vhosts.html

水平扩展 / 收缩
kubectl scale deployment nginx-deployment --replicas=4

滚动更新
kubectl create -f nginx-deployment.yaml --record

pod四个状态字段: DESIRED CURRENT UP-TO-DATE AVAILABLE
kubectl get deployments
重点关注AVAILABLE,即：既是 Running 状态，又是最新版本， 并且已经处于 Ready （健康检查正确）状态的 Pod 的个数。

实时查看 Deployment 对象的状态变化。
kubectl rollout status deployment/nginx-deployment 

查看这个 Deployment 所控制的 ReplicaSet：
kubectl get rs

修改 Deployment 有很多方法
kubectl edit deployment/nginx-deployment
kubectl rollout status deployment/nginx-deployment
kubectl describe deployment nginx-deployment

直接修改 nginx-deployment所使用的镜像:
kubectl set image deployment/nginx-deployment nginx=nginx:1.91

把整个 Deployment 回滚到上一个版本
kubectl rollout undo deployment/nginx-deployment

查看每次 Deployment 变更对应的版本
kubectl rollout history deployment/nginx-deployment 

要回滚到的指定版本的版本号
kubectl rollout history deployment/nginx-deployment --revision=2

对 Deployment 进行的每一次更新操作，都会生成一个新的 ReplicaSet 对象，会造成浪费资源。
有个方法，对 Deployment 的多次更新操作，最后只生成一个 ReplicaSet：
kubectl rollout pause deployment/nginx-deployment # 让这个 Deployment 进入了一个“暂停”状态。
...
期间的操作不会触发更新,最后只会触发一次“滚动更新”。 
...
kubectl rollout resume deploy/nginx-deployment #把这个 Deployment“恢复”回来

如何控制这些“历史”ReplicaSet 的数量呢？
spec.revisionHistoryLimit
如果设置为 0，你就再也不能做回滚操作。
还有successfulJobsHistoryLimit和failedJobsHistoryLimit


# 给master1节点添加node标签
kubectl label node master1 node-role.kubernetes.io/node=

# 给master1节点打"污点"。不允许pod在master1节点运行
kubectl taint nodes master1 node-role.kubernetes.io/master-
kubectl taint nodes master1 node-role.kubernetes.io/master=:NoSchedule

# 给master1节点打"污点"。允许pod在master1节点运行
kubectl taint nodes master1 node-role.kubernetes.io/master=:PreferNoSchedule


###################
删除集群
kubectl drain master1 --delete-local-data --force --ignore-daemonsets
kubectl drain worker1 --delete-local-data --force --ignore-daemonsets
kubectl drain worker2 --delete-local-data --force --ignore-daemonsets
kubectl delete node master1
kubectl delete node worker1
kubectl delete node worker2
kubeadm reset
iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X
ipvsadm -C

systemctl stop {docker,kubelet}

yum autoremove kubelet-1.14.3 kubectl-1.14.3 kubeadm-1.14.3
#yum purge kubelet-1.14.3 kubectl-1.14.3 kubeadm-1.14.3
ls /etc/cni/net.d/

rm -rf /var/lib/rook/*
rm -rf /var/lib/docker/*
rm -rf /var/lib/kubelet/*
rm -rf /var/lib/etcd/*
rm -rf ~/.kube/*




################
参考资料
1. 包v1beta2定义了kubeadm配置文件格式的v1beta2版本。
https://godoc.org/k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/v1beta2

2.
https://www.cnblogs.com/RainingNight/p/deploying-k8s-dashboard-ui.html

3. 通过插件集成Kubernetes
https://www.weave.works/docs/net/latest/kubernetes/kube-addon/


